{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f541af3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:15:04.838292Z",
     "iopub.status.busy": "2025-04-18T12:15:04.837924Z",
     "iopub.status.idle": "2025-04-18T12:15:23.659016Z",
     "shell.execute_reply": "2025-04-18T12:15:23.657918Z"
    },
    "papermill": {
     "duration": 18.828229,
     "end_time": "2025-04-18T12:15:23.660938",
     "exception": false,
     "start_time": "2025-04-18T12:15:04.832709",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# === Core Python & Utilities ===\n",
    "import os\n",
    "import joblib\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "\n",
    "# === Numerical & Data Handling ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dask.dataframe as dd\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "from scipy.signal import find_peaks\n",
    "\n",
    "# === Plotting ===\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# === Machine Learning ===\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# === TensorFlow / Keras ===\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.saving import register_keras_serializable\n",
    "from tensorflow.keras.layers import (\n",
    "    Layer, Conv1D, MaxPooling1D, BatchNormalization, Dropout,\n",
    "    Activation, Dense, Flatten, Reshape\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a18b9f5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:15:23.669954Z",
     "iopub.status.busy": "2025-04-18T12:15:23.669226Z",
     "iopub.status.idle": "2025-04-18T12:15:23.675932Z",
     "shell.execute_reply": "2025-04-18T12:15:23.674895Z"
    },
    "papermill": {
     "duration": 0.012747,
     "end_time": "2025-04-18T12:15:23.677634",
     "exception": false,
     "start_time": "2025-04-18T12:15:23.664887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def custom_loss_improved(y_true, y_pred):\n",
    "    # Weighted focal MSE\n",
    "    alpha = 6.0\n",
    "    gamma = 2.0\n",
    "    weight = 1 + alpha * K.abs(y_true)\n",
    "    error = y_true - y_pred\n",
    "    focal_weight = K.pow(K.abs(error), gamma)\n",
    "    mse_loss = K.mean(weight * focal_weight * K.square(error))\n",
    "\n",
    "    # Your original ranking loss (unchanged)\n",
    "    pos_mask = K.cast(K.greater(K.abs(y_true), 0.6), 'float32')\n",
    "    neg_mask = 1.0 - pos_mask\n",
    "    pos_count = K.sum(pos_mask) + K.epsilon()\n",
    "    neg_count = K.sum(neg_mask) + K.epsilon()\n",
    "    pos_mean = K.sum(K.abs(y_pred) * pos_mask) / pos_count\n",
    "    neg_mean = K.sum(K.abs(y_pred) * neg_mask) / neg_count\n",
    "    margin = 0.6\n",
    "    ranking_loss = K.relu(margin - (pos_mean - neg_mean))\n",
    "\n",
    "    return mse_loss + 0.1 * ranking_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55130ad6",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-04-18T12:15:23.686126Z",
     "iopub.status.busy": "2025-04-18T12:15:23.685764Z",
     "iopub.status.idle": "2025-04-18T12:15:25.493147Z",
     "shell.execute_reply": "2025-04-18T12:15:25.491534Z"
    },
    "papermill": {
     "duration": 1.814203,
     "end_time": "2025-04-18T12:15:25.495452",
     "exception": false,
     "start_time": "2025-04-18T12:15:23.681249",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Worn-Unworn predict XGboost model loaded successfully!\n",
      "ðŸš€ Score predict UNET model loaded successfully!\n",
      "ðŸš€ Score predict LSTM model loaded successfully!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:713: UserWarning: Skipping variable loading for optimizer 'adam', because it has 33 variables whereas the saved optimizer has 1 variables. \n",
      "  saveable.load_own_variables(weights_store.get(inner_path))\n"
     ]
    }
   ],
   "source": [
    "# Load the worn-unworn model\n",
    "xgboost_worn_unworn_model = XGBClassifier()\n",
    "xgboost_worn_unworn_model.load_model('/kaggle/input/worn-unworn-latest/worn_unworn_xgboost_model_2.json')\n",
    "print(\"ðŸš€ Worn-Unworn predict XGboost model loaded successfully!\")\n",
    "\n",
    "# load score prediction models (unet, lstm)\n",
    "\n",
    "unet_model_path = \"/kaggle/input/exp-score/unet1d_with_attention_exp_score_loss_2_earlystop.keras\"\n",
    "\n",
    "score_predict_unet_model = keras.models.load_model(\n",
    "    unet_model_path,\n",
    "    custom_objects={\n",
    "        \"custom_loss_improved\": custom_loss_improved\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\"ðŸš€ Score predict UNET model loaded successfully!\")\n",
    "\n",
    "lstm_model_path = \"/kaggle/input/exp-score/lstm_with_attention_exp_score_loss_2_earlystop.keras\"\n",
    "\n",
    "score_predict_lstm_model = keras.models.load_model(\n",
    "    lstm_model_path,\n",
    "    custom_objects={\n",
    "        \"custom_loss_improved\": custom_loss_improved\n",
    "            }\n",
    ")\n",
    "print(\"ðŸš€ Score predict LSTM model loaded successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eac94c8e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:15:25.504584Z",
     "iopub.status.busy": "2025-04-18T12:15:25.504207Z",
     "iopub.status.idle": "2025-04-18T12:15:25.574681Z",
     "shell.execute_reply": "2025-04-18T12:15:25.573708Z"
    },
    "papermill": {
     "duration": 0.077368,
     "end_time": "2025-04-18T12:15:25.576693",
     "exception": false,
     "start_time": "2025-04-18T12:15:25.499325",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load test data\n",
    "\n",
    "test_series = pd.read_parquet('/kaggle/input/child-mind-institute-detect-sleep-states/test_series.parquet')\n",
    "\n",
    "# Define a custom bucket: 18:00 to 18:00 the following day\n",
    "def assign_bucket(timestamp):\n",
    "    if timestamp.hour >= 18:\n",
    "        return timestamp.date()\n",
    "    else:\n",
    "        return (timestamp - pd.Timedelta(days=1)).date()\n",
    "\n",
    "def process_series_df(series_id):\n",
    "    # Step 1: Check if required columns exist\n",
    "    required_cols = ['series_id', 'timestamp', 'anglez', 'enmo', 'step']\n",
    "    missing_cols = [col for col in required_cols if col not in test_series.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Missing columns: {missing_cols}\")\n",
    "        return []\n",
    "\n",
    "    # Step 2: Filter the series\n",
    "    series_df = test_series[test_series['series_id'] == series_id].copy()\n",
    "    if series_df.empty:\n",
    "        return []\n",
    "\n",
    "    # Step 3: Handle missing anglez column\n",
    "    if series_df['anglez'].isna().all():\n",
    "        return []\n",
    "\n",
    "    # Step 4: Fill missing values in relevant columns (forward fill)\n",
    "    series_df[['timestamp', 'anglez', 'enmo', 'step']] = (\n",
    "        series_df[['timestamp', 'anglez', 'enmo', 'step']].ffill()\n",
    "    )\n",
    "\n",
    "    # Step 5: Convert timestamp safely\n",
    "    series_df['timestamp'] = pd.to_datetime(series_df['timestamp'], errors='coerce', utc=True)\n",
    "    if series_df['timestamp'].isna().all():\n",
    "        return []\n",
    "\n",
    "    # Step 6: Extract time-based features\n",
    "    series_df['date_hour_minute'] = series_df['timestamp'].dt.strftime('%Y-%m-%d %H:%M')\n",
    "    series_df['hour'] = series_df['timestamp'].dt.hour\n",
    "\n",
    "    # Step 7: Compute anglez value counts\n",
    "    series_df['anglez_value_counts'] = series_df['anglez'].map(series_df['anglez'].value_counts())\n",
    "\n",
    "    # Step 8: Group by minute\n",
    "    series_minute_grouped = series_df.groupby('date_hour_minute').agg({\n",
    "        'anglez': ['mean', 'std', 'median'],\n",
    "        'enmo': ['mean', 'std', 'median'],\n",
    "        'step': 'min',\n",
    "        'hour': 'first',\n",
    "        'anglez_value_counts': 'max',\n",
    "    }).reset_index()\n",
    "\n",
    "    if series_minute_grouped.empty:\n",
    "        return []\n",
    "\n",
    "    # Step 9: Rename columns\n",
    "    series_minute_grouped.columns = [\n",
    "        'date_hour_minute', 'anglez_mean', 'anglez_std', 'anglez_med',\n",
    "        'enmo_mean', 'enmo_std', 'enmo_med', 'step_min',\n",
    "        'hour', 'anglez_value_counts'\n",
    "    ]\n",
    "\n",
    "    # Step 10: Parse date_hour_minute and assign bucket\n",
    "    series_minute_grouped['date_hour_minute'] = pd.to_datetime(\n",
    "        series_minute_grouped['date_hour_minute'], errors='coerce'\n",
    "    )\n",
    "\n",
    "    series_minute_grouped['bucket'] = series_minute_grouped['date_hour_minute'].apply(assign_bucket)\n",
    "\n",
    "    return series_minute_grouped\n",
    "      \n",
    "def process(series_id):\n",
    "  print(f\"start processing {series_id}\")\n",
    "  series_minute_grouped = process_series_df(series_id)\n",
    "  if series_minute_grouped.empty:\n",
    "        print(f\"No valid data for series_id {series_id}\")\n",
    "        return []\n",
    "  series_minute_grouped['bucket'] = series_minute_grouped['bucket'].astype(str)\n",
    "\n",
    "  # Group data into buckets (dates)\n",
    "  buckets = series_minute_grouped.groupby('bucket')\n",
    "\n",
    "  # Store data for each bucket as arrays\n",
    "  bucket_arrays = []\n",
    "\n",
    "  for bucket, group in buckets:\n",
    "      step = group['step_min'].to_numpy()\n",
    "      hour = group['hour'].to_numpy()\n",
    "      enmo_values = group['enmo_mean'].to_numpy()\n",
    "      enmo_std_values = group['enmo_std'].to_numpy()\n",
    "      anglez_values = group['anglez_mean'].to_numpy()\n",
    "      anglez_values = np.clip(anglez_values, -90, 90)\n",
    "      anglez_log_values = np.log1p(anglez_values + 91)  # Apply shift by 91 so the range is 1-181 and log1p transformation\n",
    "      anglez_std_values = group['anglez_std'].to_numpy()\n",
    "      anglez_value_counts = group['anglez_value_counts'].to_numpy()\n",
    "      bucket_arrays.append({\n",
    "          'series_id': series_id,\n",
    "          'bucket': bucket,\n",
    "          'step': step,\n",
    "          'hour': hour,\n",
    "          'enmo': enmo_values,\n",
    "          'enmo_std' : enmo_std_values,\n",
    "          'anglez_log': anglez_log_values,\n",
    "          'anglez_std' : anglez_std_values,\n",
    "          'anglez_value_counts' : anglez_value_counts,\n",
    "      })\n",
    "  print(f\"finished processing {series_id}\")\n",
    "  return bucket_arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f0bbbc2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:15:25.585701Z",
     "iopub.status.busy": "2025-04-18T12:15:25.585299Z",
     "iopub.status.idle": "2025-04-18T12:15:25.701656Z",
     "shell.execute_reply": "2025-04-18T12:15:25.700299Z"
    },
    "papermill": {
     "duration": 0.123122,
     "end_time": "2025-04-18T12:15:25.703732",
     "exception": false,
     "start_time": "2025-04-18T12:15:25.580610",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start processing 038441c925bb\n",
      "finished processing 038441c925bb\n",
      "start processing 03d92c9f6f8a\n",
      "finished processing 03d92c9f6f8a\n",
      "start processing 0402a003dae9\n",
      "finished processing 0402a003dae9\n"
     ]
    }
   ],
   "source": [
    "TARGET_LENGTH = 1440\n",
    "\n",
    "def pad_to_length_edge(array, target_length=1440):\n",
    "    pad_length = target_length - len(array)\n",
    "    if pad_length > 0:\n",
    "        return np.pad(array, (0, pad_length), mode='edge')\n",
    "    return array\n",
    "\n",
    "series_ids = test_series['series_id'].unique()\n",
    "results = [process(series_id) for series_id in series_ids]\n",
    "\n",
    "flat_data = []\n",
    "for result in results:\n",
    "    for bucket in result:\n",
    "        flat_data.append({\n",
    "            \"series_id\": bucket['series_id'],\n",
    "            \"bucket\": bucket['bucket'],\n",
    "            \"step\": bucket['step'],\n",
    "            \"hour\": bucket['hour'],\n",
    "            \"enmo\": list(bucket['enmo']),\n",
    "            \"enmo_std\": list(bucket['enmo_std']),\n",
    "            \"anglez_log\": list(bucket['anglez_log']),\n",
    "            \"anglez_std\": list(bucket['anglez_std']),\n",
    "            \"anglez_value_counts\": list(bucket['anglez_value_counts']),\n",
    "        })\n",
    "\n",
    "buckets = pd.DataFrame(flat_data)\n",
    "\n",
    "# Apply padding to each feature column\n",
    "for col in ['step', 'enmo', 'hour', 'enmo_std', 'anglez_log', 'anglez_std', 'anglez_value_counts']:\n",
    "    buckets[col] = buckets[col].apply(lambda x: pad_to_length_edge(x, 1440))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9f2ec8dd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:15:25.713582Z",
     "iopub.status.busy": "2025-04-18T12:15:25.713179Z",
     "iopub.status.idle": "2025-04-18T12:15:25.745343Z",
     "shell.execute_reply": "2025-04-18T12:15:25.744491Z"
    },
    "papermill": {
     "duration": 0.039512,
     "end_time": "2025-04-18T12:15:25.747275",
     "exception": false,
     "start_time": "2025-04-18T12:15:25.707763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "buckets['anglez_log_ewma'] = buckets['anglez_log'].apply(lambda x: pd.Series(x).ewm(span=30).mean().to_numpy())\n",
    "buckets['enmo_ewma'] = buckets['enmo'].apply(lambda x: pd.Series(x).ewm(span=30).mean().to_numpy())\n",
    "for col_name in ['enmo', 'enmo_std', 'anglez_log', 'anglez_std', 'anglez_log_ewma', 'enmo_ewma']:\n",
    "  buckets[col_name] = buckets[col_name].apply(lambda arr: (arr - np.mean(arr)) / np.std(arr) if np.std(arr) != 0 else arr)\n",
    "\n",
    "\n",
    "# Efficient sliding window std\n",
    "def sliding_window_std(array, window_size=10):\n",
    "    if len(array) < window_size:\n",
    "        return np.array([0] * len(array))  # Handle edge cases\n",
    "    windows = sliding_window_view(array, window_shape=window_size)  # Create overlapping windows\n",
    "    return np.std(windows, axis=1)\n",
    "\n",
    "# Efficient sliding window mad\n",
    "def sliding_window_mad(array, window_size=10):\n",
    "    if len(array) < window_size:\n",
    "        return np.array([0] * len(array))  # Handle edge cases\n",
    "    windows = sliding_window_view(array, window_shape=window_size)  # Create overlapping windows\n",
    "    return np.mean(np.abs(np.diff(windows, axis=1)), axis=1)\n",
    "def pad_to_length_constant(array, target_length=1440):\n",
    "    pad_length = target_length - len(array)\n",
    "    if pad_length > 0:\n",
    "        return np.pad(array, (0, pad_length), mode='constant')\n",
    "    return array\n",
    "buckets['anglez_log_std'] = buckets['anglez_log'].apply(lambda x: sliding_window_std(np.array(x), window_size=30))\n",
    "buckets['anglez_log_mad'] = buckets['anglez_log'].apply(lambda x: sliding_window_mad(np.array(x), window_size=30))\n",
    "\n",
    "\n",
    "# Pad the sliding window results\n",
    "buckets['anglez_log_std'] = buckets['anglez_log_std'].apply(lambda x: pad_to_length_edge(x, 1440))\n",
    "buckets['anglez_log_mad'] = buckets['anglez_log_mad'].apply(lambda x: pad_to_length_edge(x, 1440))\n",
    "buckets['hour_sin'] = buckets['hour'].apply(lambda x: np.sin(2 * np.pi * x / 24))\n",
    "buckets['hour_cos'] = buckets['hour'].apply(lambda x: np.cos(2 * np.pi * x / 24))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4e9565f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:15:25.756379Z",
     "iopub.status.busy": "2025-04-18T12:15:25.756036Z",
     "iopub.status.idle": "2025-04-18T12:15:25.773816Z",
     "shell.execute_reply": "2025-04-18T12:15:25.771860Z"
    },
    "papermill": {
     "duration": 0.024332,
     "end_time": "2025-04-18T12:15:25.775556",
     "exception": false,
     "start_time": "2025-04-18T12:15:25.751224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of flattened X: (3, 12960)\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering: Prepare features and targets\n",
    "features = []\n",
    "for _, row in buckets.iterrows():\n",
    "    # Stack all features into a single array for the row\n",
    "    feature_matrix = np.column_stack([\n",
    "        row['anglez_log_ewma'],\n",
    "        row['anglez_log_std'],\n",
    "        row['anglez_log_mad'],\n",
    "        row['enmo_ewma'],\n",
    "        row['enmo_std'],\n",
    "        row['anglez_std'],\n",
    "        row['anglez_value_counts'],\n",
    "        row['hour_sin'],\n",
    "        row['hour_cos']\n",
    "    ])\n",
    "    features.append(feature_matrix)\n",
    "# Convert features and targets to numpy arrays\n",
    "X_test = np.array(features)  # Shape: (n_samples, 1440, n_features)\n",
    "\n",
    "# Flatten the input arrays into 1D vectors\n",
    "if X_test.size != 0:\n",
    "    X_test_flattened = X_test.reshape(X_test.shape[0], -1)  # Shape: (num_samples, features)\n",
    "    print(f\"Shape of flattened X: {X_test_flattened.shape}\")\n",
    "    worn_predictions = xgboost_worn_unworn_model.predict(X_test_flattened)\n",
    "    buckets['worn'] = worn_predictions\n",
    "    filtered_buckets = buckets[buckets['worn'] == 1]\n",
    "else:\n",
    "    filtered_buckets = buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76e17a6d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-18T12:15:25.785063Z",
     "iopub.status.busy": "2025-04-18T12:15:25.784737Z",
     "iopub.status.idle": "2025-04-18T12:15:26.259451Z",
     "shell.execute_reply": "2025-04-18T12:15:26.258050Z"
    },
    "papermill": {
     "duration": 0.481019,
     "end_time": "2025-04-18T12:15:26.260887",
     "exception": true,
     "start_time": "2025-04-18T12:15:25.779868",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3.10/contextlib.py:153: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "math domain error",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-860e15fd3447>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Predict using the trained models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msleep_awake_predictions_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_predict_unet_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0msleep_awake_predictions_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscore_predict_lstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    120\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m             \u001b[0;31m# `keras.config.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/progbar.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, current, values, finalize)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m                 \u001b[0mnumdigits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog10\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m                 \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"%\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnumdigits\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"d/%d\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mcurrent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m                 \u001b[0mbar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\x1b[1m{bar}\\x1b[0m \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: math domain error"
     ]
    }
   ],
   "source": [
    "# Extract features\n",
    "features = []\n",
    "for _, row in filtered_buckets.iterrows():\n",
    "# Stack features: each is a 1440-length array for one day\n",
    "    feature_matrix = np.column_stack([\n",
    "    row['anglez_log_ewma'],\n",
    "    row['anglez_log_std'],\n",
    "    row['anglez_log_mad'],\n",
    "    row['enmo_ewma'],\n",
    "    row['enmo_std'],\n",
    "    row['anglez_std'],\n",
    "    row['anglez_value_counts'],\n",
    "    row['hour_sin'],\n",
    "    row['hour_cos']\n",
    "    ])\n",
    "    features.append(feature_matrix)\n",
    "\n",
    "X = np.array(features)  # Ensure shape: (n_samples, 1440, n_features)\n",
    "\n",
    "# Predict using the trained models\n",
    "sleep_awake_predictions_1 = score_predict_unet_model.predict(X)\n",
    "sleep_awake_predictions_2 = score_predict_lstm_model.predict(X)\n",
    "\n",
    "# Assign predictions to the DataFrame  \n",
    "filtered_buckets[\"predicted_target\"] = [(0.6*sleep_awake_predictions_1[i].flatten()+0.4*sleep_awake_predictions_2[i].flatten()) for i in range(len(filtered_buckets))]\n",
    "\n",
    "# Save results\n",
    "filtered_buckets.to_parquet(\"buckets_for_post_processing.parquet\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cb59db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-16T15:10:07.992521Z",
     "iopub.status.busy": "2025-03-16T15:10:07.992034Z",
     "iopub.status.idle": "2025-03-16T15:10:08.040723Z",
     "shell.execute_reply": "2025-03-16T15:10:08.039489Z",
     "shell.execute_reply.started": "2025-03-16T15:10:07.992486Z"
    },
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load data using Dask\n",
    "df = dd.read_parquet('buckets_for_post_processing.parquet')\n",
    "\n",
    "threshold_onset, threshold_wakeup = 0.05, 0.05\n",
    "\n",
    "# Step adjustment based on empirical insights\n",
    "def adjust_step(step, event_type):\n",
    "    step = int(step)  # ensure it's a standard Python integer\n",
    "    if step % 12 in [0, 6]:\n",
    "        step -= 1\n",
    "    e15 = event_type[0] + str((step // 12) % 15)\n",
    "    if e15 in ['o1', 'o8', 'o9', 'w0', 'w4', 'w8', 'w12']:\n",
    "        step -= 12\n",
    "    elif e15 in ['o4', 'o12', 'w1', 'w5', 'w9', 'w13']:\n",
    "        step += 12\n",
    "    return step\n",
    "\n",
    "# Comprehensive event extraction accounting for sleep breaks\n",
    "def process_partition(partition_df):\n",
    "    min_sleep_duration, max_sleep_duration = 60 * 12, 1000 * 12\n",
    "    max_sleep_break = 30 * 12  # 30 minutes in steps\n",
    "    submission_rows = []\n",
    "\n",
    "    for idx, row in partition_df.iterrows():\n",
    "        predicted_scores = row['predicted_target']\n",
    "        if predicted_scores is None:\n",
    "          print(idx)\n",
    "          continue  # Skip rows where predicted_scores are None\n",
    "\n",
    "        predicted_scores = np.array(predicted_scores).flatten()\n",
    "\n",
    "        steps = np.array(row['step'], dtype=np.int64) \n",
    "\n",
    "        onset_peaks, _ = find_peaks(predicted_scores, height=threshold_onset)\n",
    "        wakeup_peaks, _ = find_peaks(-predicted_scores, height=threshold_wakeup)\n",
    "\n",
    "        onset_steps_adjusted = [adjust_step(steps[idx], 'onset') for idx in onset_peaks]\n",
    "        wakeup_steps_adjusted = [adjust_step(steps[idx], 'wakeup') for idx in wakeup_peaks]\n",
    "\n",
    "        onset_scores = predicted_scores[onset_peaks]\n",
    "        wakeup_scores = predicted_scores[wakeup_peaks]\n",
    "\n",
    "        for onset_step, onset_score in zip(onset_steps_adjusted, onset_scores):\n",
    "            for wakeup_step, wakeup_score in zip(wakeup_steps_adjusted, wakeup_scores):\n",
    "                duration = wakeup_step - onset_step\n",
    "                if duration < 0:\n",
    "                    continue\n",
    "                if min_sleep_duration <= duration <= max_sleep_duration or (0 < duration <= max_sleep_break):\n",
    "                  if abs(onset_score) > threshold_onset:\n",
    "                      submission_rows.append({\n",
    "                          'series_id': str(row['series_id']),\n",
    "                          'step': int(onset_step),\n",
    "                          'event': 'onset',\n",
    "                          'score': float(abs(onset_score))\n",
    "                      })\n",
    "\n",
    "                  if abs(wakeup_score) > threshold_wakeup:\n",
    "                      submission_rows.append({\n",
    "                          'series_id': str(row['series_id']),\n",
    "                          'step': int(wakeup_step),\n",
    "                          'event': 'wakeup',\n",
    "                          'score': float(abs(wakeup_score))\n",
    "                      })\n",
    "    if submission_rows:\n",
    "        return pd.DataFrame(submission_rows).drop_duplicates()\n",
    "    else:\n",
    "        return pd.DataFrame(columns=['series_id', 'step', 'event', 'score'])\n",
    "\n",
    "# Apply processing to each partition\n",
    "results = df.map_partitions(process_partition, meta={\n",
    "    'series_id': str,\n",
    "    'step': int,\n",
    "    'event': str,\n",
    "    'score': float\n",
    "})\n",
    "\n",
    "# Compute results, assign row_id, and save\n",
    "submission_df = results.reset_index(drop=True).compute()\n",
    "submission_df['row_id'] = submission_df.index\n",
    "submission_df = submission_df[['row_id', 'series_id', 'step', 'event', 'score']]\n",
    "submission_df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6589269,
     "sourceId": 53666,
     "sourceType": "competition"
    },
    {
     "datasetId": 6884413,
     "sourceId": 11050647,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6884447,
     "sourceId": 11050696,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6905032,
     "sourceId": 11078689,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6924630,
     "sourceId": 11107250,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6930268,
     "sourceId": 11115020,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6930465,
     "sourceId": 11115282,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6944431,
     "sourceId": 11134283,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6944632,
     "sourceId": 11134578,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6958736,
     "sourceId": 11153308,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6966878,
     "sourceId": 11213102,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7008964,
     "sourceId": 11223630,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7009687,
     "sourceId": 11223704,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7024394,
     "sourceId": 11251303,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7061802,
     "sourceId": 11293889,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 7074421,
     "sourceId": 11406471,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 26.505592,
   "end_time": "2025-04-18T12:15:27.989181",
   "environment_variables": {},
   "exception": true,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-04-18T12:15:01.483589",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
